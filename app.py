import streamlit as st
import pandas as pd
import altair as alt

import matplotlib.pyplot as plt
import os
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel, BertConfig


import numpy as np

model_path = "./models/best_model_bert.bin"
# 18ä¸ªè¯„ä»·ç»´åº¦ (é¡ºåºå¿…é¡»ä¸è®­ç»ƒæ—¶æ ‡ç­¾é¡ºåºä¸€è‡´)
LABEL_COLUMNS = [
    'Location#Transportation', 'Location#Downtown', 'Location#Easy_to_find',
    'Service#Queue', 'Service#Hospitality', 'Service#Parking', 'Service#Timely',
    'Price#Level', 'Price#Cost_effective', 'Price#Discount',
    'Ambience#Decoration', 'Ambience#Noise', 'Ambience#Space', 'Ambience#Sanitary',
    'Food#Portion', 'Food#Taste', 'Food#Appearance', 'Food#Recommend'
]

# ç»´åº¦ä¸­æ–‡æ˜ å°„
ASPECT_MAP = {
    'Food#Taste': 'å‘³é“/å£æ„Ÿ', 'Food#Portion': 'åˆ†é‡', 'Food#Appearance': 'å¤–è§‚', 'Food#Recommend': 'æ€»ä½“æ¨è',
    'Price#Level': 'ä»·æ ¼æ°´å¹³', 'Price#Cost_effective': 'æ€§ä»·æ¯”', 'Price#Discount': 'æŠ˜æ‰£ä¼˜æƒ ',
    'Service#Timely': 'ç‰©æµ/æ—¶æ•ˆ', 'Service#Hospitality': 'æœåŠ¡æ€åº¦', 'Service#Queue': 'æ’é˜Ÿ',
    'Service#Parking': 'åœè½¦', 'Location#Transportation': 'äº¤é€šä¾¿åˆ©æ€§', 'Location#Downtown': 'æ˜¯å¦å¸‚ä¸­å¿ƒ',
    'Location#Easy_to_find': 'ä½ç½®å¥½æ‰¾', 'Ambience#Decoration': 'è£…ä¿®/æ°›å›´', 'Ambience#Noise': 'å™ªéŸ³æƒ…å†µ',
    'Ambience#Space': 'ç©ºé—´å¤§å°', 'Ambience#Sanitary': 'å«ç”ŸçŠ¶å†µ'
}

# æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„ (æ ¹æ®ä½ è®­ç»ƒæ—¶çš„å®šä¹‰ä¿®æ”¹ï¼Œé€šå¸¸æ˜¯ 0:æœªæåŠ, 1:è´Ÿé¢, 2:ä¸­æ€§, 3:æ­£é¢)
# å‡è®¾ä½ çš„æ¨¡å‹è¾“å‡º 4 ä¸ªç±»åˆ«
ID2LABEL = {0: 'æœªæåŠ', 1: 'è´Ÿé¢', 2: 'ä¸­æ€§', 3: 'æ­£é¢'}


# å®šä¹‰æ¨¡å‹ç»“æ„ (å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´)
# å‡è®¾æ˜¯åŸºäº BERT çš„å¤šä»»åŠ¡åˆ†ç±»æ¨¡å‹
class AspectBasedSentimentAnalysisModel(nn.Module):
    def __init__(self, n_classes=4, n_aspects=18):
        super(AspectBasedSentimentAnalysisModel, self).__init__()
        # è¿™é‡Œä½¿ç”¨ bert-base-chineseï¼Œå› ä¸ºå®ƒæœ€å¸¸ç”¨
        self.bert = BertModel.from_pretrained('bert-base-chinese')
        self.drop = nn.Dropout(p=0.3)
        # è¾“å‡ºå±‚ï¼š18 ä¸ªç»´åº¦ï¼Œæ¯ä¸ªç»´åº¦ 4 ä¸ªç±»åˆ«
        self.out = nn.Linear(self.bert.config.hidden_size, n_aspects * n_classes)
        self.n_classes = n_classes
        self.n_aspects = n_aspects

    def forward(self, input_ids, attention_mask):
        pooled_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )[1]
        output = self.drop(pooled_output)
        logits = self.out(output)
        # é‡å¡‘å½¢çŠ¶ä¸º (batch_size, n_aspects, n_classes)
        return logits.view(-1, self.n_aspects, self.n_classes)


@st.cache_resource
def load_model():
    """åŠ è½½æ¨¡å‹æƒé‡å’Œåˆ†è¯å™¨"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 1. åŠ è½½åˆ†è¯å™¨ (è‡ªåŠ¨ä¸‹è½½ bert-base-chinese)
    try:
        tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
    except Exception as e:
        st.error(f"åˆ†è¯å™¨åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æœ¬åœ°ç¼“å­˜: {e}")
        return None, None, None

    # 2. å®ä¾‹åŒ–æ¨¡å‹æ¶æ„
    model = AspectBasedSentimentAnalysisModel(n_classes=4, n_aspects=18)

    # 3. åŠ è½½è®­ç»ƒå¥½çš„æƒé‡ (.bin æ–‡ä»¶)
    #model_path = "./models/best_model_state.bin"
    if os.path.exists(model_path):
        try:
            # map_location ç¡®ä¿åœ¨ CPU ä¸Šä¹Ÿèƒ½åŠ è½½ GPU è®­ç»ƒçš„æ¨¡å‹
            state_dict = torch.load(model_path, map_location=device)
            model.load_state_dict(state_dict)
            model.to(device)
            model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            return tokenizer, model, device
        except Exception as e:
            st.error(f"æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥ï¼Œè¯·ç¡®è®¤æ¨¡å‹æ¶æ„æ˜¯å¦åŒ¹é…: {e}")
            return None, None, None
    else:
        st.warning(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}ï¼Œé¢„æµ‹åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")
        return None, None, None


def predict_sentiment(text, tokenizer, model, device):
    """æ‰§è¡Œé¢„æµ‹"""
    if not tokenizer or not model:
        return {}

    # é¢„å¤„ç†
    inputs = tokenizer(
        text,
        max_length=512,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )

    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)

    # æ¨ç†
    with torch.no_grad():
        logits = model(input_ids, attention_mask)  # Output: (1, 18, 4)
        predictions = torch.argmax(logits, dim=2)  # Output: (1, 18)

    # è§£æç»“æœ
    results = {}
    preds_list = predictions[0].cpu().numpy()

    for idx, aspect in enumerate(LABEL_COLUMNS):
        label_id = preds_list[idx]
        label_str = ID2LABEL[label_id]
        # åªè®°å½•æåŠçš„ç»´åº¦ (é'æœªæåŠ')
        if label_str != 'æœªæåŠ':
            results[ASPECT_MAP.get(aspect, aspect)] = label_str

    return results


# ==========================================
# 1. é¡µé¢åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(
    page_title="è¯„è®ºè§‚ç‚¹æŒ–æ˜ç³»ç»Ÿ",
    layout="wide"
)

# è®¾ç½® Matplotlib ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'PingFang SC']
plt.rcParams['axes.unicode_minus'] = False


# ==========================================
# 2. æ•°æ®åŠ è½½å‡½æ•°
# ==========================================
@st.cache_data
def load_data(file_path):
    try:
        df = pd.read_csv(file_path)
        if 'time' in df.columns:
            df['time'] = pd.to_datetime(df['time'], errors='coerce')
        return df
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return None


# ==========================================
# 3. ä¾§è¾¹æ  (Sidebar)
# ==========================================
st.sidebar.title(" ç³»ç»Ÿæ§åˆ¶é¢æ¿")

uploaded_file = st.sidebar.file_uploader("ä¸Šä¼ åˆ†æç»“æœ CSV", type=['csv'])
default_file = "result/prediction_result.csv"

if uploaded_file is not None:
    df = load_data(uploaded_file)
    st.sidebar.success(" å·²åŠ è½½ä¸Šä¼ æ–‡ä»¶")
elif os.path.exists(default_file):
    df = load_data(default_file)
    st.sidebar.info(f" å·²åŠ è½½é»˜è®¤æ–‡ä»¶: {default_file}")
else:
    # å¦‚æœæ²¡æœ‰æ•°æ®æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„ DataFrame ä»¥ä¾¿ UI èƒ½æ˜¾ç¤ºï¼ˆè‡³å°‘é¢„æµ‹åŠŸèƒ½èƒ½ç”¨ï¼‰
    df = pd.DataFrame(columns=LABEL_COLUMNS + ['content'])
    st.sidebar.warning("æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼ä»…é¢„æµ‹åŠŸèƒ½å¯ç”¨ã€‚")

if not df.empty and 'time' in df.columns:
    st.sidebar.subheader("æ•°æ®ç­›é€‰")
    min_date = df['time'].min().date() if pd.notnull(df['time'].min()) else None
    max_date = df['time'].max().date() if pd.notnull(df['time'].max()) else None
    if min_date and max_date:
        start_date, end_date = st.sidebar.date_input("é€‰æ‹©æ—¶é—´èŒƒå›´", [min_date, max_date])

#é™æ€å›¾
def show_static_evaluation_plots():
    st.subheader(" æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°å¯è§†åŒ–")

    # å®šä¹‰å›¾ç‰‡è·¯å¾„ (è¯·ç¡®ä¿è¿™äº›å›¾ç‰‡åœ¨ä½ çš„é¡¹ç›®æ ¹ç›®å½•ä¸‹)
    image_paths = {
        "Loss & Accuracy": "./result/loss_accuracy_curve.png",
        "Bi-LSTM": "./result/Bi-lstm.png",
        "BERT- F1": "./result/bert_detailed_performance.png",
        "18ç»´åº¦ç›¸å…³æ€§çƒ­åŠ›å›¾": "./result/aspect_correlation_18x18.png",
        "æ··æ·†çŸ©é˜µ": "./result/confusion_matrix_result.png"
    }

    # åˆ›å»ºé€‰é¡¹å¡ï¼Œè®©å±•ç¤ºæ›´æ•´æ´
    tab1, tab2, tab3 = st.tabs([" è®­ç»ƒè¿‡ç¨‹", " æ¨¡å‹å¯¹æ¯”", " æ·±åº¦æŒ–æ˜"])

    with tab1:
        st.markdown("### Bert-Loss & Accuracy")
        if os.path.exists(image_paths["Loss & Accuracy"]):
            st.image(image_paths["Loss & Accuracy"], caption="è®­ç»ƒé›†ä¸éªŒè¯é›†çš„ Loss/Accuracy å˜åŒ–",
                     use_container_width=True)
        if os.path.exists(image_paths["BERT- F1"]):
            st.image(image_paths["BERT- F1"], caption="BERT æ¨¡å‹è¯¦ç»†è®­ç»ƒæŒ‡æ ‡", use_container_width=True)
        else:
            st.warning("æœªæ‰¾åˆ°è®­ç»ƒæ›²çº¿å›¾ç‰‡ (loss_accuracy_curve.png)ï¼Œè¯·å…ˆè¿è¡Œç»˜å›¾è„šæœ¬ã€‚")

    with tab2:
        st.markdown("### Bi-LSTM")
        if os.path.exists(image_paths["Bi-LSTM"]):
            st.image(image_paths["Bi-LSTM"], caption="Bi-LSTM (Accuracy, F1, Time)",
                     use_container_width=True)
            st.success("ç»“è®ºï¼šBERT åœ¨å‡†ç¡®ç‡å’Œ F1 åˆ†æ•°ä¸Šæ˜¾è‘—ä¼˜äº Bi-LSTMï¼Œä½†è®­ç»ƒæ—¶é—´è¾ƒé•¿ã€‚")
        else:
            st.info("æœªæ‰¾åˆ°å¯¹æ¯”å›¾ (model_comparison_result.png)ã€‚")

    with tab3:
        st.markdown("### ç»†ç²’åº¦æƒ…æ„ŸæŒ–æ˜")

        # çƒ­åŠ›å›¾
        # ğŸ”´ ä¿®æ”¹ç‚¹ï¼šuse_column_width -> use_container_width
        if os.path.exists(image_paths["18ç»´åº¦ç›¸å…³æ€§çƒ­åŠ›å›¾"]):
            st.image(image_paths["18ç»´åº¦ç›¸å…³æ€§çƒ­åŠ›å›¾"], caption="18ä¸ªè¯„ä»·ç»´åº¦çš„å…±ç°ç›¸å…³æ€§çŸ©é˜µ",
                     use_container_width=True)
            st.markdown("> **è§£è¯»**ï¼šçº¢è‰²åŒºåŸŸè¡¨ç¤ºä¸¤ä¸ªè¯é¢˜ç»å¸¸åŒæ—¶å‡ºç°ï¼ˆå¦‚â€œä»·æ ¼â€å’Œâ€œæ€§ä»·æ¯”â€ï¼‰ï¼Œè“è‰²è¡¨ç¤ºäº’æ–¥ã€‚")

        st.divider()
        if os.path.exists(image_paths["æ··æ·†çŸ©é˜µ"]):
            st.image(image_paths["æ··æ·†çŸ©é˜µ"], caption="æƒ…æ„Ÿåˆ†ç±»æ··æ·†çŸ©é˜µ", use_container_width=True)
# ==========================================
#ä¸»ç•Œé¢ (Main Dashboard)

st.title("è¯„è®ºè§‚ç‚¹æŒ–æ˜ä¸åˆ†æ")
show_static_evaluation_plots()
st.markdown("åŸºäº BERT çš„ç»†ç²’åº¦æƒ…æ„Ÿåˆ†æç»“æœå±•ç¤º")


# ç›´æ¥è¯»å– bin æ–‡ä»¶è¿›è¡Œç¡¬æ ¸å±•ç¤º
def analyze_model_file(model_path):
    st.subheader(" æ¨¡å‹æ·±åº¦è¯Šæ–­ (åŸºäºæƒé‡æ–‡ä»¶)")

    if not os.path.exists(model_path):
        st.error(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        return

    # 1. åŠ è½½æ¨¡å‹æƒé‡
    try:
        # map_location='cpu' ä¿è¯åœ¨æ²¡æœ‰ GPU çš„ç”µè„‘ä¸Šä¹Ÿèƒ½æ‰“å¼€
        state_dict = torch.load(model_path, map_location='cpu')
    except Exception as e:
        st.error(f"æ¨¡å‹è¯»å–å¤±è´¥: {e}")
        return

    # 2. åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
    col1, col2, col3 = st.columns(3)

    # è®¡ç®—æ€»å‚æ•°é‡
    total_params = sum(p.numel() for p in state_dict.values())
    file_size = os.path.getsize(model_path) / (1024 * 1024)  # è½¬æ¢ä¸º MB

    with col1:
        st.metric("æ¨¡å‹æ–‡ä»¶å¤§å°", f"{file_size:.2f} MB")
    with col2:
        st.metric("æ€»å‚æ•°é‡ (Parameters)", f"{total_params / 1000000:.2f} M (ç™¾ä¸‡)")
    with col3:
        st.metric("åŒ…å«å±‚æ•° (Tensors)", f"{len(state_dict)} å±‚")

    st.info(f"æˆåŠŸè¯»å–æ¨¡å‹æƒé‡æ–‡ä»¶ï¼š`{os.path.basename(model_path)}`ã€‚ä»¥ä¸‹æ˜¯æ¨¡å‹å†…éƒ¨å‚æ•°çš„å¯è§†åŒ–åˆ†æã€‚")

    # 3. æƒé‡åˆ†å¸ƒå¯è§†åŒ– (Weight Histogram)
    # è¿™å±•ç¤ºäº†ä½ è®­ç»ƒçš„æ¨¡å‹å‚æ•°æ˜¯å¦â€œå¥åº·â€ï¼ˆé€šå¸¸åº”å‘ˆæ­£æ€åˆ†å¸ƒï¼‰
    st.write("####  æ ¸å¿ƒå±‚æƒé‡åˆ†å¸ƒå¯è§†åŒ–")

    # é€‰å–å‡ ä¸ªå…³é”®å±‚è¿›è¡Œå±•ç¤º
    target_layers = {
        "è¯åµŒå…¥å±‚ (Embeddings)": "bert.embeddings.word_embeddings.weight",
        "ç¼–ç å™¨ç¬¬1å±‚ (Encoder Layer 1)": "bert.encoder.layer.0.output.dense.weight",
        "åˆ†ç±»è¾“å‡ºå±‚ (Classifier)": "out.weight"
    }

    # åˆ›å»º matplotlib ç”»å¸ƒ
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    for idx, (label, layer_name) in enumerate(target_layers.items()):
        ax = axes[idx]
        if layer_name in state_dict:
            # è·å–æƒé‡å¹¶è½¬ä¸º numpy
            weights = state_dict[layer_name].cpu().numpy().flatten()

            # ç»˜åˆ¶ç›´æ–¹å›¾
            ax.hist(weights, bins=50, color='#3182bd', alpha=0.7)
            ax.set_title(label)
            ax.set_xlabel("æƒé‡å€¼")
            ax.set_ylabel("æ•°é‡")

            # æ˜¾ç¤ºå‡å€¼å’Œæ–¹å·®
            mean_val = np.mean(weights)
            std_val = np.std(weights)
            ax.text(0.95, 0.95, f'$\mu={mean_val:.4f}$\n$\sigma={std_val:.4f}$',
                    transform=ax.transAxes, ha='right', va='top',
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        else:
            ax.text(0.5, 0.5, "æœªæ‰¾åˆ°è¯¥å±‚\nå¯èƒ½æ¨¡å‹ç»“æ„ä¸åŒ¹é…", ha='center')

    st.pyplot(fig)

    # 4. è§£é‡Š
    st.markdown("""
    > **å¤§æ•°æ®ç†è®ºåˆ†æ**ï¼š
    > ä¸Šå›¾å±•ç¤ºäº†æ¨¡å‹å†…éƒ¨ç¥ç»å…ƒçš„æ¿€æ´»çŠ¶æ€ã€‚
    > * **è¯åµŒå…¥å±‚**ï¼šå±•ç¤ºäº†æ¨¡å‹å¯¹ä¸­æ–‡è¯æ±‡çš„åˆå§‹ç†è§£ã€‚
    > * **æ­£æ€åˆ†å¸ƒ**ï¼šæƒé‡å‘ˆç°é’Ÿå½¢æ›²çº¿ï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ï¼Œè¯´æ˜æ¨¡å‹è®­ç»ƒæ”¶æ•›æƒ…å†µè‰¯å¥½ï¼Œæ²¡æœ‰å‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±çš„é—®é¢˜ã€‚
    """)


# ==========================================
# åœ¨ä¸»ç¨‹åºä¸­è°ƒç”¨ (æ”¾åœ¨ st.title ä¹‹åå³å¯)
# ==========================================

# æ·»åŠ ä¸€ä¸ªå¤é€‰æ¡†æ¥å¼€å¯è¿™ä¸ªç¡¬æ ¸æ¨¡å¼ï¼Œé¿å…é¡µé¢å¤ªä¹±
if st.checkbox("æ˜¾ç¤ºæ¨¡å‹å†…éƒ¨æƒé‡åˆ†æ (Debug Mode)", value=True):
    analyze_model_file(model_path)
# --- ç¬¬ä¸€éƒ¨åˆ†ï¼šå…³é”®æŒ‡æ ‡ (KPI) ---
if not df.empty:
    st.subheader("1. å…³é”®æ•°æ®æ¦‚è§ˆ")
    col1, col2, col3, col4 = st.columns(4)

    total_comments = len(df)
    # æ£€æŸ¥ä¸­æ–‡åˆ—å 'åˆ†æ•°'
    if 'åˆ†æ•°' in df.columns:
        # ç»Ÿè®¡ 4åˆ†å’Œ5åˆ† çš„æ¯”ä¾‹
        positive_rate = (df[df['åˆ†æ•°'] >= 4].shape[0] / total_comments) * 100
        metric_label = "äº”æ˜Ÿå¥½è¯„ç‡"

    # æˆ–è€…æ£€æŸ¥è‹±æ–‡åˆ—å 'score'
    elif 'score' in df.columns:
        positive_rate = (df[df['score'] >= 4].shape[0] / total_comments) * 100
        metric_label = "äº”æ˜Ÿå¥½è¯„ç‡"
    elif 'Food#Taste' in df.columns:
        # ç»Ÿè®¡è§‰å¾—â€œå‘³é“å¥½â€çš„æ¯”ä¾‹
        pos_taste = df[df['Food#Taste'] == 'æ­£é¢'].shape[0]
        positive_rate = (pos_taste / total_comments) * 100
        metric_label = "å‘³é“æ»¡æ„åº¦"
    elif 'Food#Recommend' in df.columns:
        pos_rec = df[df['Food#Recommend'] == 'æ­£é¢'].shape[0]
        positive_rate = (pos_rec / total_comments) * 100
        metric_label = "æ¨èæŒ‡æ•° (åŸºäºæ¨¡å‹)"
    else:
        positive_rate = 0
        metric_label = "æš‚æ— è¯„åˆ†æ•°æ®"

    with col1:
        st.metric("æ€»è¯„è®ºæ•°", f"{total_comments} æ¡")
    with col2:
        st.metric(metric_label, f"{positive_rate:.1f}%")
    with col3:
        counts = {}
        for col in LABEL_COLUMNS:
            if col in df.columns:
                counts[col] = df[df[col] != 'æœªæåŠ'].shape[0]
        if counts:
            top_aspect = max(counts, key=counts.get)
            st.metric("æœ€çƒ­è®¨è®ºç‚¹", ASPECT_MAP.get(top_aspect, top_aspect))
        else:
            st.metric("æœ€çƒ­è®¨è®ºç‚¹", "æš‚æ— æ•°æ®")
    with col4:
        st.metric("æ¨¡å‹åˆ†æç»´åº¦", "18 ä¸ª")

    st.divider()

    # --- ç¬¬äºŒéƒ¨åˆ†ï¼šå¤šç»´æƒ…æ„Ÿåˆ†æå›¾è¡¨ ---
    st.subheader("2. å±æ€§ç»´åº¦æƒ…æ„Ÿåˆ†å¸ƒ")

    plot_data = []
    for col in LABEL_COLUMNS:
        if col in df.columns:
            vc = df[col].value_counts()
            for sentiment in ['æ­£é¢', 'è´Ÿé¢', 'ä¸­æ€§']:
                count = vc.get(sentiment, 0)
                if count > 0:
                    plot_data.append({
                        'ç»´åº¦': ASPECT_MAP.get(col, col),
                        'åŸå§‹ç»´åº¦': col,
                        'æƒ…æ„Ÿ': sentiment,
                        'è¯„è®ºæ•°': count
                    })

    df_plot = pd.DataFrame(plot_data)

    if not df_plot.empty:
        chart = alt.Chart(df_plot).mark_bar().encode(
            x=alt.X('ç»´åº¦', sort='-y', title='å•†å“å±æ€§ç‰¹å¾'),
            y=alt.Y('è¯„è®ºæ•°', title='è§‚ç‚¹æ•°é‡'),
            color=alt.Color('æƒ…æ„Ÿ',
                            scale=alt.Scale(domain=['æ­£é¢', 'ä¸­æ€§', 'è´Ÿé¢'], range=['#28a745', '#ffc107', '#dc3545']),
                            legend=alt.Legend(title="æƒ…æ„Ÿææ€§")),
            tooltip=['ç»´åº¦', 'æƒ…æ„Ÿ', 'è¯„è®ºæ•°']
        ).properties(height=400).interactive()

        st.altair_chart(chart, use_container_width=True)
    else:
        st.warning("æš‚æ— ç›¸å…³æƒ…æ„Ÿæ•°æ®å¯å±•ç¤ºã€‚")

    st.info(" **å›¾è¡¨è§£è¯»**ï¼šç»¿è‰²ä»£è¡¨æ­£é¢è¯„ä»·ï¼Œçº¢è‰²ä»£è¡¨è´Ÿé¢è¯„ä»·ã€‚æŸ±å­è¶Šé«˜ï¼Œä»£è¡¨ç”¨æˆ·è®¨è®ºè¯¥å±æ€§çš„æ¬¡æ•°è¶Šå¤šã€‚")

    col_a, col_b = st.columns(2)

    with col_a:
        st.subheader("3. ç”¨æˆ·å…³æ³¨ç‚¹æ’è¡Œ")
        if not df_plot.empty:
            aspect_counts = df_plot.groupby('ç»´åº¦')['è¯„è®ºæ•°'].sum().reset_index().sort_values('è¯„è®ºæ•°', ascending=False)
            bar_chart = alt.Chart(aspect_counts).mark_bar().encode(
                x=alt.X('è¯„è®ºæ•°', title='æåŠæ¬¡æ•°'),
                y=alt.Y('ç»´åº¦', sort='-x', title='å•†å“å±æ€§ç‰¹å¾'),
                color=alt.value('#3182bd')
            ).properties(height=300)
            st.altair_chart(bar_chart, use_container_width=True)
        else:
            st.write("æš‚æ— æ•°æ®")

    with col_b:
        st.subheader("4. è´Ÿé¢è¯„ä»·é‡ç¾åŒº")
        if not df_plot.empty:
            neg_counts = df_plot[df_plot['æƒ…æ„Ÿ'] == 'è´Ÿé¢'].sort_values('è¯„è®ºæ•°', ascending=False).head(5)
            if not neg_counts.empty:
                neg_chart = alt.Chart(neg_counts).mark_bar().encode(
                    x=alt.X('ç»´åº¦', sort='-y', title='å•†å“å±æ€§ç‰¹å¾'),
                    y=alt.Y('è¯„è®ºæ•°', title='è´Ÿé¢è¯„ä»·æ•°é‡'),
                    color=alt.value('#dc3545')
                ).properties(height=300)
                st.altair_chart(neg_chart, use_container_width=True)
            else:
                st.success("æš‚æ— æ˜¾è‘—çš„è´Ÿé¢è¯„ä»·èšé›†ï¼")
        else:
            st.write("æš‚æ— æ•°æ®")

    st.divider()




#åœ¨çº¿é¢„æµ‹
st.subheader("6. åœ¨çº¿æƒ…æ„Ÿé¢„æµ‹")
st.markdown("è¾“å…¥ä¸€æ®µå•†å“è¯„ä»·ï¼ŒåŸºäºåŠ è½½çš„ BERT æ¨¡å‹å®æ—¶é¢„æµ‹å…¶åŒ…å«çš„ç»†ç²’åº¦æƒ…æ„Ÿã€‚")

tokenizer, model, device = load_model()

with st.form("predict_form"):
    user_input = st.text_area("è¯·è¾“å…¥è¯„è®ºæ–‡æœ¬ï¼š", placeholder="ä¾‹å¦‚ï¼šè¿™å®¶åº—å‘³é“ä¸é”™ï¼Œä½†æ˜¯ä»·æ ¼æœ‰ç‚¹è´µï¼Œæ’é˜Ÿä¹Ÿå¾ˆä¹…ã€‚",
                              height=100)
    submit_btn = st.form_submit_button("å¼€å§‹é¢„æµ‹ ")

if submit_btn and user_input:
    if not model:
        st.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•é¢„æµ‹ã€‚è¯·æ£€æŸ¥ best_model_state1.bin æ–‡ä»¶ã€‚")
    else:
        with st.spinner("æ¨¡å‹æ­£åœ¨åˆ†æä¸­..."):
            results = predict_sentiment(user_input, tokenizer, model, device)

        if results:
            st.success("åˆ†æå®Œæˆï¼æ£€æµ‹åˆ°ä»¥ä¸‹è§‚ç‚¹ï¼š")

            # ä½¿ç”¨åˆ—å¸ƒå±€å±•ç¤ºç»“æœ
            # ä¸ºäº†ç¾è§‚ï¼Œæ¯è¡Œæ˜¾ç¤º 3 ä¸ªç»“æœ
            items = list(results.items())
            rows = [items[i:i + 3] for i in range(0, len(items), 3)]

            for row in rows:
                cols = st.columns(3)
                for idx, (aspect, sentiment) in enumerate(row):
                    color = "gray"
                    if sentiment == 'æ­£é¢':
                        color = "green"
                    elif sentiment == 'è´Ÿé¢':
                        color = "red"
                    elif sentiment == 'ä¸­æ€§':
                        color = "orange"

                    cols[idx].markdown(f"**{aspect}**")
                    cols[idx].markdown(f":{color}[**{sentiment}**]")
        else:
            st.info("æ¨¡å‹æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„è¯„ä»·ç»´åº¦ï¼ˆæ‰€æœ‰ç»´åº¦å‡ä¸º'æœªæåŠ'ï¼‰ã€‚")

st.divider()

# --- ç¬¬äº”éƒ¨åˆ†ï¼šæ•°æ®é€è§†ä¸ä¸‹è½½ ---
if not df.empty:
    st.subheader("7. åŸå§‹æ•°æ®æŸ¥è¯¢")

    df_display = df.copy()
    rename_dict = {k: v for k, v in ASPECT_MAP.items() if k in df_display.columns}
    df_display.rename(columns=rename_dict, inplace=True)

    st.dataframe(df_display, use_container_width=True)

    csv = df.to_csv(index=False).encode('utf-8-sig')
    st.download_button(
        " ä¸‹è½½åˆ†ææŠ¥å‘Š (CSV)",
        csv,
        "analysis_report.csv",
        "text/csv",
        key='download-csv'
    )